
[paths]
train = training_data/train.spacy
dev = training_data/dev.spacy

[system]
seed = 42

[nlp]
lang = "es"
pipeline = ["transformer","ner"]
batch_size = 32

[components]

[components.transformer]
factory = "transformer"
max_batch_items = 4096

[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v3"
name = "xlm-roberta-base"
tokenizer_config = {"use_fast":true}
transformer_config = {"output_attentions":false,"output_hidden_states":false}

[components.ner]
factory = "ner"

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
tok2vec = {"@architectures":"spacy-transformers.TransformerListener.v3","grad_factor":1.0}
hidden_width = 128
maxout_pieces = 2
use_upper = false

[training]
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
max_epochs = 20
patience = 5
eval_frequency = 1000
dropout = 0.1

[training.optimizer]
@optimizers = "AdamW.v1"
learn_rate = 0.00005
beta1 = 0.9
beta2 = 0.999
weight_decay = 0.01

[corpora]

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}